I developed a code to induce hallucinations in the CLIP Vision Encoder, deliberately poisoning its perception. This manipulation aims to distort the contextual understanding of the Language Model (LLM) by the CLIP model. By injecting these hallucinations, the code alters how the CLIP model interprets and integrates visual information, potentially leading to unexpected or biased outputs when combined with textual inputs processed by the LLM. This approach explores the vulnerabilities and adaptability of multimodal models, highlighting complexities in ensuring robust and unbiased interactions between vision and language components in AI systems.
